{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1UzS3lR03zvLsV8lMFN9QDH58UVABsBMo","timestamp":1709370948807}],"mount_file_id":"1UzS3lR03zvLsV8lMFN9QDH58UVABsBMo","authorship_tag":"ABX9TyMHdIwX8czHmau8sKTSaSF9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"E68NZ6xAiOtb"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"code","source":["#define image size and batch size\n","IMG_SIZE=224\n","BATCH_SIZE=32"],"metadata":{"id":"f3mQ6V_4i2Z6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_datagen=ImageDataGenerator(\n","    rescale=1./255,\n","    validation_split=0.2\n",")"],"metadata":{"id":"IumQSWs5jAHl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"allZPsOBjFLM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#creating training data with above parameters\n","train_generator=train_datagen.flow_from_directory(\n","r\"/content/drive/MyDrive/Brain_Tumor_Detection/Train\",\n","    target_size=(IMG_SIZE, IMG_SIZE),\n","    batch_size=BATCH_SIZE,\n","    class_mode='binary',\n","    subset='training'\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jq4dqNEKkSqz","executionInfo":{"status":"ok","timestamp":1709304863106,"user_tz":-330,"elapsed":7769,"user":{"displayName":"Jyothsna Darla","userId":"14616404553274733227"}},"outputId":"2bb31b96-572b-47b4-9fe0-931781caeb62"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2439 images belonging to 2 classes.\n"]}]},{"cell_type":"code","source":["#creating validation data\n","val_generator=train_datagen.flow_from_directory(\n","r\"/content/drive/MyDrive/Brain_Tumor_Detection/Train\",\n","    target_size=(IMG_SIZE,IMG_SIZE),\n","    batch_size=BATCH_SIZE,\n","    class_mode='binary',\n","    subset='validation'\n",")"],"metadata":{"id":"geSrGi5Ol_c0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709304882474,"user_tz":-330,"elapsed":583,"user":{"displayName":"Jyothsna Darla","userId":"14616404553274733227"}},"outputId":"d425906f-387d-4968-9578-d08392ad99e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 609 images belonging to 2 classes.\n"]}]},{"cell_type":"code","source":["#Define the model\n","model = keras.Sequential([\n","    layers.Conv2D(32,(3,3),activation='relu',input_shape=(IMG_SIZE,IMG_SIZE,3)),\n","    layers.MaxPooling2D((2,2)),\n","    layers.Conv2D(64, (3,3),activation='relu'),\n","    layers.MaxPooling2D((2,2)),\n","    layers.Conv2D(128,(3,3),activation='relu'),\n","    layers.MaxPooling2D((2,2)),\n","    layers.Flatten(),\n","    layers.Dense(128,activation='relu'),\n","    layers.Dense(1,activation='sigmoid')\n","])"],"metadata":{"id":"r7U6I8Gnq0zm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#compile the model\n","model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"],"metadata":{"id":"mrahsXNVrK7q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.fit(train_generator,validation_data=val_generator,epochs=5)"],"metadata":{"id":"t25X3bbyrPNq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709306968628,"user_tz":-330,"elapsed":2046481,"user":{"displayName":"Jyothsna Darla","userId":"14616404553274733227"}},"outputId":"8887b9b4-2f01-4896-fb13-929c56f17508"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","77/77 [==============================] - 752s 10s/step - loss: 0.5454 - accuracy: 0.7774 - val_loss: 0.2638 - val_accuracy: 0.8818\n","Epoch 2/5\n","77/77 [==============================] - 314s 4s/step - loss: 0.2447 - accuracy: 0.8979 - val_loss: 0.1408 - val_accuracy: 0.9425\n","Epoch 3/5\n","77/77 [==============================] - 296s 4s/step - loss: 0.1208 - accuracy: 0.9561 - val_loss: 0.1404 - val_accuracy: 0.9557\n","Epoch 4/5\n","77/77 [==============================] - 316s 4s/step - loss: 0.0673 - accuracy: 0.9795 - val_loss: 0.0406 - val_accuracy: 0.9901\n","Epoch 5/5\n","77/77 [==============================] - 314s 4s/step - loss: 0.0295 - accuracy: 0.9930 - val_loss: 0.0275 - val_accuracy: 0.9918\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7b5ec0b76b60>"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["model.save(\"Model.h5\",\"label.txt\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RucnYvdJpHhc","executionInfo":{"status":"ok","timestamp":1709307147338,"user_tz":-330,"elapsed":551,"user":{"displayName":"Jyothsna Darla","userId":"14616404553274733227"}},"outputId":"b8fde87c-51b2-4e83-e278-c55ee4535308"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.models import load_model\n","from tensorflow.keras.preprocessing import image\n","import numpy as np"],"metadata":{"id":"mLpBB2JYdOH6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model=load_model('/content/Model.h5')"],"metadata":{"id":"7Uhy67mHdPi5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_image_path=''\n","img=image.load_img(test_image_path,target_size(224,224))\n","img_array=image.img_to_array(img)\n","img_array=np.expand_dims(img_array,axis=0)\n","\n","#add batch dimension\n","img_array/=255.   #Normalize the pixel value\n","#make predictions\n","prediction=model.predict(img_array)\n","#print the prediction\n","print(prediction)"],"metadata":{"id":"CEF7viwidUO-","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"error","timestamp":1709387189477,"user_tz":-330,"elapsed":44,"user":{"displayName":"Jyothsna Darla","userId":"14616404553274733227"}},"outputId":"f7e7d062-6a73-4fad-89bf-19e4328ec074"},"execution_count":1,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"(unicode error) 'unicodeescape' codec can't decode bytes in position 3-4: truncated \\UXXXXXXXX escape (<ipython-input-1-aad8cd649b5f>, line 1)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-aad8cd649b5f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    test_image_path='\"C:\\Users\\jyoth\\Downloads\\archive(2)\\chest\\chest_xray\\train\\PNEUMONIA\"'\u001b[0m\n\u001b[0m                                                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 3-4: truncated \\UXXXXXXXX escape\n"]}]},{"cell_type":"code","source":["if prediction<0.5:\n","  print(\"prediction: no tumor(probability:)\",prediction[0][0])\n","else:\n","  print(\"prediction: tumor(probability:)\",prediction[0][0])"],"metadata":{"id":"fFxXrkjmdaP5"},"execution_count":null,"outputs":[]}]}